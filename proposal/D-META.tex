\documentclass{sig-alternate}

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{url}
\usepackage{float}
% \usepackage{cite}
% \usepackage{math}
\usepackage{array}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

\usepackage[small,bf]{caption}

\usepackage{color}
\definecolor{darkgreen}{rgb}{0.0,0.5,0.0}
\definecolor{red}{rgb}{1.0,0.0,0.0}

\newcommand{\todo}[1]{ \textcolor{red}{\bf #1}}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{multirow}
\usepackage{ifpdf}

\begin{document}


\title{D-META Grand Challenge}
\subtitle{Data sets for Multimodal Evaluation of Tasks and Annotations}
% \subtitle{}
% %
% % You need the command \numberofauthors to handle the 'placement
% % and alignment' of the authors beneath the title.
% %
% % For aesthetic reasons, we recommend 'three authors at a time'
% % i.e. three 'name/affiliation blocks' be placed beneath the title.
% %
% % NOTE: You are NOT restricted in how many 'rows' of
% % "name/affiliations" may appear. We just ask that you restrict
% % the number of 'columns' to three.
% %
% % Because of the available 'opening page real-estate'
% % we ask you to refrain from putting more than six authors
% % (two rows with three columns) beneath the article title.
% % More than six makes the first-page appear very cluttered indeed.
% %
% % Use the \alignauthor commands to handle the names
% % and affiliations for an 'aesthetic maximum' of six authors.
% % Add names, affiliations, addresses for
% % the seventh etc. author(s) as the argument for the
% % \additionalauthors command.
% % These 'additional authors' will be output/set for you
% % without further effort on your part as the last section in
% % the body of your article BEFORE References or any Appendices.
%

\numberofauthors{3}
\author{
\alignauthor Xavier Alameda-Pineda \\
\affaddr{Perception Team,} 
\affaddr{655, Av. Europe, 38334 Montbonnot,} 
\affaddr{INRIA Rh\^one-Alpes, University of Grenoble, France} \\
\email{xavier.alameda-pineda@inria.fr}
% Adjunct Professor of Language Technology
% University of Helsinki
\alignauthor{Dirk Heylen} \\
\affaddr{Human Media Interaction,} 
\affaddr{PO BOX 217, 7500 AE Enschede,}
\affaddr{University of Twente, The Netherlands}\\
\email{d.k.j.heylen@utwente.nl}
\alignauthor Kristiina Jokinen \\
\affaddr{Department of Behavioural Sciences,}
\affaddr{PO BOX 9, FIN-00014,}
\affaddr{University of Helsinki, Finland}\\
\email{kristiina.jokinen@helsinki.fi}
} 


\toappear{}

% \clubpenalty=10000
% \widowpenalty = 10000

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Machine learning-based challenges do not exist in the Multimodal Interaction community. In this paper we propose the
\texttt{D-META} challenge to set up the basis for comparison, analysis, and further development of multimodal data and
multimodal interactive systems. The main goal of this challenge is to foster research and development in multimodal
communication and to enable further development of algorithms and techniques for building various multimodal
applications. Held by two pillars, method benchmarking and annotation evaluation, the \texttt{D-META} challenge
envisions a starting point for transparent and publicly available application and annotation evaluation on multimodal
data sets. In this paper we describe the motivations of the challenge, the conceptual background in which the challenge
is based as well as several practical issues necessary to set up the \texttt{D-META} challenge.
\end{abstract}
\section{Introduction}
Machine learning-based tasks have been gainning importance in the last decades. This lead to several interesting
challenges such as the PASCAL Visual Object Classes \cite{PascalVOC}, the Active Learning challenge
\cite{ActiveLearning} or the Speller \cite{Speller}. In addition, several tool have been developed to evaluate the
annotation also in the form of a challenge such as \cite{Give} or \cite{SDS}. Because the fields they are related to
(visual object recognition, active learning from unlabelled data, spell correction, ...) are mature, the
evaluation systems are accepted by the community and everybody uses them. However, all these challenges are monomodal,
meaning that the data is acquired with one type of sensor.\vspace{0.3cm}

Similar challenges do not exist in the Multimodal Interaction community. However, in this paper we suggest to setup
such a challenge so as to allow comparison, analysis, and further development of multimodal data and multimodal
interactive systems. We believe that the challenge has a positive impact on the field and that it will help to
increase our understanding of the varied tasks and activities related to area. This will provide for more robust and 
natural automatic systems, as well as guidelines towards shared methods and concepts so at to support comparison and
evaluation of such systems. Indeed, setting up the D-META challenge is demanding and even ambitious: the diversity of
targetted applications, the acquired data sets and their annotations, make the task a challenge in itself. Some issues
need to be addressed when proposing such challenges:
\begin{description}
 \item [Data sets] A few data sets need to be selected. These should be as much versatile as possible to in order to
handle different multimodal applications. In addition, the data sets should be publicly available and free, in order to
guarantee a fair participation in the challenge.
 \item [Evaluation metrics] They should be set up by the challenge, providing for a faire evaluation between different
methods.
 \item [Annotations] They should be provided with the data set. We are interested also in evaluating these annotations
to converge towards consistently annotated data sets.
\end{description}
Following the Spoken Dialogue Challenge, we want to keep the first round fairly simple and clear, so as to obtain
feedback from the community and to gather experience for growing the challenge, in the coming years, to cover more
varied activities, tools and annotation levels.\vspace{0.3cm}

In the next, we describe the main properties of \texttt{D-META}. After the plan for participation, we outline
the proposed schedule. Before detailing the format of the challenge, we give some hints about who are we. Finally the
references are provided.


\section{Description}
The main goal of the \texttt{D-META} Grand Challenge is to foster research and development in multimodal communication
and to enable further development of algorithms and techniques for building various multimodal applications. Although
we do not aim at providing a unified framework (cf. EMMA), we will start with a particular data set and a precise
annotation scheme, in order to benchmark methods targeted at diverse multimodal applications as well as evaluating
annotation systems. We hope that this framework will function as a starting point for researchers in the Multimodal
Interaction community to share their data sets in a structured fashion.\vspace{0.3cm}

The \texttt{D-META} challenge has these two fundamental and complementary pillars, method benchmarking and
annotation evaluation, and several objectives:
\begin{itemize}
\item To motivate and encourage research and development so as to understand the needs, requirements, possibilities, and
challenges that are to be addressed when developing multimodal interactive systems for real-world applications and
working systems.
\item To target ecologically valid data for automatic system development.
\item To provide evaluation criteria for assessing the performance of a system and to compare several systems with each
other.
\item To increase interest in the rich and multifacedted research area of multimodal interaction.
\end{itemize}

On one hand, since multimodal data sets are expensive (both in acquisition and annotation), they are often partially
annotated, just to satisfy the targetted application. In addition, the provided annotation may have place for
improvement. Providing annotations on data sets will ease two tasks: 1) the genesis of
cross-validated annotations and 2) the use of such data sets by the community. Hence, methods targetting multimodal
applications may be benchmarked using several common data sets in a transparent way.\vspace{0.3cm}

On the other hand, the set up of an application-oriented challenge will encourage researchers to work with other data
sets than theirs. Thus, annotations of these data sets will have to be rich and precise. Systematic evaluation
techniques will be required to provide for such annotations which, in turn, will be needed to have a clear
benchmarking of methods targetting multimodal applications.\vspace{0.3cm}

In summary, we envision \texttt{D-META} as a two coupled-branch provide a starting point for research and development
aiming transparent and publicly available application and annotation evaluation on multimodal data sets.

\section{Participation plan}
In order to set up this Grand Challenge, we chosed a few existing publicly-available multimodal data sets useful
for different multimodal applications and/or several annotation evaluations. This choice will be public within the call
for papers. Authors may submit their methods/benchmarks for applications and/or their systems/comparisons for
annotations and evaluation.\vspace{0.3cm}

A list of possible data sets, applications and annotations is:
\begin{description}
 \item [RAVEL] The RAVEL data set was presented in \cite{Ravel}. Concerning the multimodal community, this data set
targets Audio-Visual person detecting and tracking and Audio-Visual robot gesture recognition.
 \item [NOMCO] The NOMCO data set was presented in \cite{Paggio10}. It is used to verify empirically
how gestures and speech interact in feedback, turn management and sequencing.
\end{description}

We expect papers covering areas such as: (i) applications of an algorithm to dataset(s) to solve a precise taks, (ii)
benchmark of several algorithms using the same dataset(s), (iii) extensions of the annotation scheme with new relevant
features, (iv) applications of the data to an automatic system, (v) discussions on ecologically valid datasets and (vi)
position papers of how to organise the next challenge. This will be detailed in the call for papers (CFP).

\section{Schedule}
The schedule has the following important dates:
\begin{description}
 \item[30-Jan-2012] Web site set up and the CFP\\
 The site will be ready by the end of January and it will contain the main ambitions of \texttt{D-META}, what do we
envision and organisation issues as well as the CFP. All the information will be published there.
 \item[19-Mar-2012] Data set choice publicly available\\
 The data set choice will be ready before April, and it will explain what are we looking for (more precisely).
 \item[15-Jun-2012] Paper deadline
 \item[24-Aug-2012] Author notification
 \item[14-Sep-2012] Camera-ready
 \item[Oct-2012] Presentations at ICMI'2012
\end{description}

\section{Program Comitee}
The program comitee is right now work in progress, but it will contain people with background covering the following
fields:
\begin{itemize}
 \item multimodal interactive systems,
 \item machine learning,
 \item ecollogical data,
 \item multimodal signal processing and
 \item automatic system development.
\end{itemize}
People would be contacted in the following weeks, and the final comitte will be published in the web site.

\section{Organizers' bio}
\begin{description}
 \item [Xavier Alameda-Pineda] is PhD candidate at Perception Team, at INRIA Rh\^one-Alpes. He graduated in
Telecommunication Engineering and Mathematics at Technical University of Catalonia. After a Masters program in computer
vision, graphics and robotics, he started a PhD in Audio-Visual fusion. His main research interest lie in the field of
Audio-Visual machine perception (detection, localization and tracking).
 \item [Dirk Heylen] is Professor at University of Twente since September 2011, in the Human Media Interaction group.
Before, he was visiting professors at CNRS. He is primarily interested in modelling the way conversations influence and
are influenced by the emotions of the participants. This involves building richer dialogue management models and in
interpreting multi-modal acts.
 \item [Kristiina Jokinen] is adjunct Professor of language technology at the University of Helsinki and visiting
professor of intelligent user interfaces at the University of Tartu. She strongly collaborates with
several researchers in Japan, specially from Doshisha Universtity in Kyoto. She is project director of NOMCO
(University of Helsinki) and Multimodal Interaction (University of Tartu), secretary of SIGDial and the author of two
books \cite{Jokinen09} and \cite{Jokinen09a}.	
\end{description}

\section{Format}
Since the domain of the challenge we are proposing is quite vast, we would like to have a full-day workshop at
ICMI'2012. The challenge authors think that it is very important to keep the website up to date. Hence, we will need
web support. We would also like to have some funding for the coffee breaks and for the lunch of the attendees.

\bibliographystyle{plain}
\bibliography{D-META}

\end{document}
