\documentclass{sig-alternate}

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{url}
\usepackage{float}
% \usepackage{cite}
% \usepackage{math}
\usepackage{array}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

\usepackage[small,bf]{caption}

\usepackage{color}
\definecolor{darkgreen}{rgb}{0.0,0.5,0.0}
\definecolor{red}{rgb}{1.0,0.0,0.0}

\newcommand{\todo}[1]{ \textcolor{red}{\bf #1}}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{multirow}
\usepackage{ifpdf}

\begin{document}


\title{D-META Grand Challenge}
\subtitle{Data sets for Multimodal Evaluation of Tasks and Annotations}
% \subtitle{}
% %
% % You need the command \numberofauthors to handle the 'placement
% % and alignment' of the authors beneath the title.
% %
% % For aesthetic reasons, we recommend 'three authors at a time'
% % i.e. three 'name/affiliation blocks' be placed beneath the title.
% %
% % NOTE: You are NOT restricted in how many 'rows' of
% % "name/affiliations" may appear. We just ask that you restrict
% % the number of 'columns' to three.
% %
% % Because of the available 'opening page real-estate'
% % we ask you to refrain from putting more than six authors
% % (two rows with three columns) beneath the article title.
% % More than six makes the first-page appear very cluttered indeed.
% %
% % Use the \alignauthor commands to handle the names
% % and affiliations for an 'aesthetic maximum' of six authors.
% % Add names, affiliations, addresses for
% % the seventh etc. author(s) as the argument for the
% % \additionalauthors command.
% % These 'additional authors' will be output/set for you
% % without further effort on your part as the last section in
% % the body of your article BEFORE References or any Appendices.
%

\numberofauthors{3}
\author{
\alignauthor Xavier Alameda-Pineda \\
\affaddr{Perception Team,} 
\affaddr{655, Av. Europe, 38334 Montbonnot,} 
\affaddr{INRIA Rh\^one-Alpes, University of Grenoble, France} \\
\email{xavier.alameda-pineda@inria.fr}
% Adjunct Professor of Language Technology
% University of Helsinki
\alignauthor{Dirk Heylen} \\
\affaddr{Human Media Interaction,} 
\affaddr{PO BOX 217, 7500 AE Enschede,}
\affaddr{University of Twente, The Netherlands}\\
\email{d.k.j.heylen@utwente.nl}
\alignauthor Kristiina Jokinen \\
\affaddr{Department of Behavioural Sciences,}
\affaddr{PO BOX 9, FIN-00014,}
\affaddr{University of Helsinki, Finland}\\
\email{kristiina.jokinen@helsinki.fi}
} 


\toappear{}

% \clubpenalty=10000
% \widowpenalty = 10000

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
aasdf
\end{abstract}
\section{Introduction}
Machine learning-based tasks have been gainning importance in the last decades. This lead to several interesting
challenges such as the PASCAL Visual Object Classes \cite{PascalVOC}, the Active Learning challenge
\cite{ActiveLearning} or the Speller \cite{Speller}. In addition, several tool have been developed to evaluate the
annotation also in the form of challenge (see \cite{Give}). Becuase the fields they are related to (visual object
recognition, active learning from unlabelled data and spell correction respectively) are mature, the evaluation systems
are accepted by the community and everybody uses them. However, all these challenges are monodal, meaning that the data
is acquired with one type of sensor.\vspace{0.3cm}

Such machine learning-like challenge does not exist in the Multimodal Interaction community. This is because the
targetted applications are much more diverse, and so they are the acquired data sets and their annotations. Some issues
need to be addressed when proposing such challenges:
\begin{description}
 \item [Data sets] A few data sets need to be selected. These should be as much versatile as possible to in order to
handle different multimodal applications. In addition, the data sets should be publicly available and free, in order to
guarantee a fair participation in the challenge.
 \item [Evaluation metrics] They should be set up by the challenge, providing for a faire evaluation between different
methods.
 \item [Annotations] They should be provided with the data set. We are interested also in evaluating these annotations
to converge towards consistently annotated data sets.
\end{description}
However, the materials and methods exist to make possible challenges related to multimodal applications.\vspace{0.3cm}




\section{Description}
The long-term goal of the \texttt{D-META} Grand Challenge is to provide an unified framework of Multimodal data sets,
with precises annotations and related applications, in order to benchmark methods targeting diverse multimodal
applications as well as evaluating annotation systems. This framework will allow researchers in the Multimodal
Interaction community to share their data sets in a structured fashion.\vspace{0.3cm}

Hold by these two fundamental pillars, method benchmarking and annotation evaluation, the \texttt{D-META} challenge
will set up the basis to converge towaards the long-term goal of an unified framework. Notice how the two main pillars
are coupled and complementary.\vspace{0.3cm}

On one hand, since multimodal data sets are expensive (both in acquisition and annotation), they are often partially
annotated, just to satisfy the targetted application. In addition, the provided annotation may have place for
improvement. Providing a framework that unifies annotations on data sets will ease two tasks: 1) the genesis of
cross-validated annotations and 2) the use of such data sets by the community. Hence, methods targetting multimodal
applications may be benchmarked using several common data sets in a transparent way.\vspace{0.3cm}

On the other hand, the set up of an application-oriented challenge will encourage researchers to work with other data
sets than thirs. Thus, annotations of these data sets will have to be rich and precise. Systematic evaluation
techniquest will be required to provide for such annotations which, in turn, will be needed to have a clear
benchmarking of methods targetting multimodal applications.\vspace{0.3cm}

In summary, we envision \texttt{D-META} as a two coupled-branch long-term genesis of an unified framework aiming
transparent and publicly available application and annotation evaluation on multimodal data sets.

\section{Participation plan and schedule}
In order to set up this Grand Challenge, wi will choose a few existing publicly-available multimodal data sets useful
for different multimodal applications and/or several annotation evaluations. This choice will be public within the call
for papers. Authors may submit their methods/Benchmarks for applications and/or their systems/comparisons for
annotations and itd evaluation.\vspace{0.3cm}

The schedule has the following important dates:
\begin{description}
 \item[30-Jan-2012] Web site set up.\\
 The site will be ready by the end of January and it will contain the main ambitions of \texttt{D-META}, what do we
envision and organisation issues. All the information will be published there.
 \item[19-Mar-2012] Call for papers (with data set choice)\\
 The call for papers will be ready before April, and it will explain what are we looking for (more precisely) and what
is more important, the data sets to work with.
 \item[15-Jun-2012] Paper deadline
 \item[24-Aug-2012] Author notification
 \item[14-Sep-2012] Camera-ready
 \item[Oct-2012] Presentations at ICMI'2012
\end{description}


\section{Organizers' bio}
\subsection{Xavier Alameda-Pineda}
\subsection{Kristiina Jokinen}
\subsection{Dirk Heylen}

\section{Funding}

\section{Format}

\bibliographystyle{plain}
\bibliography{D-META}

\end{document}
