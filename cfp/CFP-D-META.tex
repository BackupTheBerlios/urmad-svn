\documentclass[a4paper]{article}

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{url}
\usepackage{float}
% \usepackage{cite}
% \usepackage{math}
\usepackage{array}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algorithmic}

% \usepackage{times}
% \usepackage{helvet}
\usepackage{courier}

\usepackage[small,bf]{caption}

\usepackage{anysize}
\marginsize{2cm}{2cm}{2cm}{2cm}

\usepackage{color}
\definecolor{darkgreen}{rgb}{0.0,0.5,0.0}
\definecolor{red}{rgb}{1.0,0.0,0.0}

\newcommand{\todo}[1]{ \textcolor{red}{\bf #1}}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{multirow}
\usepackage{ifpdf}

\pagestyle{empty}
\begin{document}

\begin{figure}
\vspace{-3.3cm}
\hspace{-2.2cm}
\includegraphics[width=1.25\textwidth]{Banner.jpg}\vspace{0.2cm}\\
\end{figure}


\begin{center}
{\LARGE
\texttt{D-META} Grand Challenge -- Call for papers
} 
\end{center}
\vspace{0.5cm}



% \numberofauthors{3}
% \author{
% \alignauthor Xavier Alameda-Pineda \vspace{0.2cm}\\
% \affaddr{Perception Team,} 
% \affaddr{655, Av. Europe, 38334 Montbonnot,} 
% \affaddr{INRIA Rh\^one-Alpes, University of Grenoble, France} \vspace{0.2cm}\\
% \email{\normalsize xavier.alameda-pineda@inria.fr}
% % Adjunct Professor of Language Technology
% % University of Helsinki
% \alignauthor{Dirk Heylen} \vspace{0.2cm}\\
% \affaddr{Human Media Interaction,} 
% \affaddr{PO BOX 217, 7500 AE Enschede,}
% \affaddr{University of Twente, The Netherlands}\vspace{0.2cm}\\
% \email{\normalsize d.k.j.heylen@utwente.nl}
% \alignauthor Kristiina Jokinen \vspace{0.2cm}\\
% \affaddr{Department of Behavioural Sciences,}
% \affaddr{PO BOX 9, FIN-00014,}
% \affaddr{University of Helsinki, Finland}\vspace{0.2cm}\\
% \email{\normalsize kristiina.jokinen@helsinki.fi}
% } 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}
\begin{minipage}{0.92\textwidth}
{\small
The \texttt{D-META} Grand Challenge aims to set up the basis for comparison, analysis, and further improvement of
multimodal data annotations and multimodal interactive systems. The main goal of this Grand Challenge is to foster
research and development in multimodal communication and to further elaborate algorithms and techniques for building
various multimodal applications. Held by two coupled pillars, method benchmarking and annotation evaluation, the
\texttt{D-META} challenge envisions a starting point for transparent and publicly available application and annotation
evaluation on multimodal data sets.}
\end{minipage}
\end{center}
\vspace{0.5cm}


\begin{center}
\textbf{\Large Main features}\vspace{0.2cm}\\

\fbox{\begin{minipage}{0.85\textwidth}
Paper dead line: \textbf{31-Jul-2012}.\vspace{0.1cm}\\
Summary paper with all the resutls published at IEEE/ACM \textbf{ICMI'2012 Proceedings}.
\end{minipage}}
\end{center}

% \section{Introduction}
% Machine learning-based tasks have been gaining importance in the last decades. This led to several interesting
% challenges such as the PASCAL Visual Object Classes \cite{PascalVOC}, the Active Learning challenge
% \cite{ActiveLearning} or the Speller \cite{Speller}. In addition, several tools have been developed to evaluate the
% annotation also in the form of a challenge such as the Natural Language Generation Challenge \cite{Give} or
% the Spoken Dialogue Challenge \cite{SDS}. Because the fields they are related to
% (visual object recognition, active learning from unlabeled data, spell correction, ...) are mature, the
% evaluation systems are accepted by the community. However, all these challenges are monomodal,
% meaning that the data is acquired with one type of sensor.\vspace{0.4cm}
% 
% Similar challenges do not exist in the Multimodal Interaction community. Because we believe that this kind of
% challenges are constructive, in this paper we suggest to setup a challenge so as to allow comparison,
% analysis, and further development of multimodal data annotations and multimodal interactive systems. We believe that
% the
% challenge has a positive impact on the field and that it will help to increase our understanding of the varied tasks
% and
% activities related to the area. In addition, we are enthusiastic about it since this will provide for more robust and 
% natural automatic systems, as well as guidelines towards shared methods and concepts so at to support comparison and
% evaluation of such systems. Indeed, setting up the \texttt{D-META} challenge is demanding and even ambitious: the
% diversity of targeted applications, the acquired data sets and their annotations, make the task a challenge in itself.
% Some issues need to be addressed when proposing such challenges:
% \begin{description}
%  \item [Data sets] A few data sets need to be selected. These should be as much versatile as possible to in order to
% handle different multimodal applications. In addition, the data sets should be publicly available and free, in order
% to
% guarantee a fair participation in the challenge.
%  \item [Evaluation metrics] They should be set up by the challenge in the call for papers, although we are looking
% forward to the evolution of these metrics in order to be accepted by the community.
%  \item [Annotations] They should be provided with the data set. We are interested also in evaluating these annotations
% to converge towards consistently annotated data sets.
% \end{description}
% 
% Following the Spoken Dialogue Challenge, we want to keep the first round fairly simple and clear, so as to obtain
% feedback from the community and to gather experience for growing the challenge, in the coming years, to cover more
% varied activities, tools and annotation levels.\vspace{0.4cm}
% 
% In the next, we describe the main properties of \texttt{D-META}. After the plan for participation, we outline
% the proposed schedule. Before detailing the format of the challenge, we give some hints about who we are. At the end,
% the references are provided.
% 
% 
% \section{Description}
% The main goal of the \texttt{D-META} Grand Challenge is to foster research and development in multimodal communication
% and to enable further development of algorithms and techniques for building various multimodal applications. Although
% we do not aim at providing an unified framework (cf. EMMA), we will start with a particular data set and a precise
% annotation scheme, in order to benchmark methods targeted at diverse multimodal applications as well as evaluating
% annotation systems. We hope that this framework will function as a starting point for researchers in the Multimodal
% Interaction community to share their data sets in a structured fashion.\vspace{0.4cm}
% 
% The \texttt{D-META} challenge has these two fundamental and complementary pillars, method benchmarking and
% annotation evaluation, and several objectives:
% \begin{itemize}
% \item To motivate and encourage research and development so as to understand the needs, requirements, possibilities,
% and
% challenges that are to be addressed when developing multimodal interactive systems for real-world applications and
% working systems.
% \item To target ecologically valid data for automatic system development.
% \item To provide evaluation criteria for assessing the performance of a system and to compare several systems with
% each
% other.
% \item To increase interest in the rich and multifaceted research area of multimodal interaction.
% \end{itemize}
% 
% On one hand, since multimodal data sets are expensive (both in acquisition and annotation), they are often partially
% annotated, just to satisfy the targeted application. Hence, the provided annotation may have place for
% improvement. Providing annotations on data sets will ease two tasks: 1) the genesis of
% cross-validated annotations and 2) the use of such data sets by the community. Thus, methods targeting multimodal
% applications may be benchmarked using several common data sets in a transparent way.\vspace{0.4cm}
% 
% On the other hand, the set up of an application-oriented challenge will encourage researchers to work with other data
% sets than theirs. Thus, annotations of these data sets will have to be rich and precise. Systematic evaluation
% techniques will be required to provide for such annotations which, in turn, will be needed to have a clear
% benchmarking of methods targeting multimodal applications.\vspace{0.4cm}
% 
% In summary, we envision \texttt{D-META} as a two coupled-branch challenge providing a starting point for research and
% development aiming transparent and publicly available application and annotation evaluation on multimodal data sets.

\section*{Scope}
The \texttt{D-META} challenged is organized by tasks. Each tasks to be solved has one (or more) associated data set. In
order to evaluate the performance on each task, a Golden Standard will be provided. Such standard consists of (partial)
annotations and sets the ground truth over the data set concerning the targetted task.\vspace{0.2cm}\\
Authors may submit their methods/benchmarks for applications and/or their systems/comparisons for
annotations and evaluation. We expect papers covering areas such as: (i) applications of an algorithm to a data
set(s) to solve precise tasks, (ii) benchmark of several algorithms using the same data set(s), (iii) extensions of the
annotation scheme with new relevant features, (iv) applications of the data to an automatic system, (v) discussions on
ecologically valid data sets and (vi) position papers of how to organise the next challenge.\vspace{0.2cm}\\
In the following, the tasks are outlined, further in the text, they are detailed.
\begin{description}
 \item [AVGR] Recognize gesture by means of the vision and the audio.
 \item [AVSR] Detect, localize and track multiple speakers using audio-visual information.
 \item [------] \todo{Set up your task.}
\end{description}

\section*{Important dates}
The schedule has the following important dates:
\begin{center}
\begin{tabular}{rl}
 \textbf{19-Mar-2012} & Data set annotation is released \vspace{0.1cm}\\
 \textbf{31-Jul-2012} & Paper deadline \vspace{0.1cm}\\
 \textbf{24-Aug-2012} & Author notification \vspace{0.1cm}\\
 \textbf{14-Sep-2012} & Camera-ready \vspace{0.1cm}\\
    \textbf{Oct-2012} & Work presented at \texttt{D-META}'12 \vspace{0.1cm}\\
\end{tabular}
\end{center}

\section*{Format}
The papers should be formatted following the ACM/IEEE format as in the ICMI 2012 proceedings. No more than six pages
long, the manuscripts should contain the motivation, a brief description of the benchmarked methods, and an extensive
discussion of the obtained results. No description of the data sets is needed, but the citation to the reference papers.
A summary paper collecting the results presented by the authors will be published together with the ICMI proceedings.

\section*{Detailed tasks}
The tasks outlined before are explained in detail in the following.

\subsection*{AVGR - Audio Visual Gesture Recognition}
The aim of this task is to recognize gestures from audio and video. In the Ravel data set \cite{Ravel}, the ``Robot
Gesture'' scenario was conceived for this particular purpose. Indeed, several actors/actresses performed gestures such
as ``point'', ``yes'', ``not'', ... The task is the isolated recognition of these robot gestures. 

\paragraph{Evaluation metric} In order to have a common way to evaluate different methods, the ``confusion matrix''
should be used when targetting this task. A confusion matrix for $N$ classes is an $N\times N$ matrix with its $ij$-th
element ($i$-th row, $j$-column) means: how many instances os class $i$ the method recognized as class $j$. The
confusion matrix for all the gestures should be provided. Obviously, testing and training subsets should be different.
We reccomend a leave-one-out strategy for evaluation.

\subsection*{AVSR - Audio Visual Speaker Recognition}
The aim of the task is to detect, localize and track speakers from audio-visual sequences. The data used are some
scenarios of the ``interaction'' part in the Ravel data set \cite{Ravel}. See the data set web
site\footnote{\url{http://ravel.humavips.eu}} for more information on which sequences to use.

\paragraph{Evaluation metric} To evaluate the results, the (euclidean) distance matrix between the detected speakers and
the ground-truth speakers. Next, one ground-truth cluster should be associated at most to each detected cluster. The
assignment procedure is as follows. For each detected cluster its closest ground-truth cluster is computed. If it is
not closer than a threshold $\tau_{\textrm{loc}}$ it is marked as \textit{false positive}, otherwise the detected
cluster is assigned to the ground-truth cluster. Then, for each ground-truth cluster the number of detected
clusters are assigned to it is checked. If there is none, the ground-truth cluster is marked as \textit{missing
detection}. Otherwise, the closest detected cluster becomes the \textit{true positive} and the remaining ones
become \textit{false positives}. Recall, precision and accuracy values should be shown in tables (and occasionaly in
figures also) for values of $\tau_\textrm{loc}$ in the range 1cm -- 50cm.

\bibliographystyle{plain}
\bibliography{CFP-D-META}

\end{document}
